#!/bin/bash

#SBATCH -J CSET-%jobID%
#SBATCH -o /scratch/cunningham/causet%jobID%/causet%jobID%.log
#SBATCH -e /scratch/cunningham/causet%jobID%/causet%jobID%.err
#SBATCH -p gpu
#SBATCH -N %nnodes%
#SBATCH --exclusive
#SBATCH --gres=%gres%
#SBATCH --mem=%memory%
#SBATCH --parsable
#SBATCH -D /scratch/cunningham/causet%jobID%

###########################
#(C) Will Cunningham 2016 #
#         DK Lab          #
# Northeastern University #
###########################

homedir=$CAUSET_HOME
work=$SCRATCH/causet%jobID%
if [[ %readjobid% -ne 0 && %readgraphid% -eq 0 ]] ; then
  usegraphlist=1
else
  usegraphlist=0
fi

# Set up peripheral files
sed "s:@jobID@:%jobID%:g;s:@readjobid@:%readjobid%:g;s:@readgraphid@:%readgraphid%:g;s:@partition@:%partition%:g" < $homedir/cluster/slurm/setup/causet_setup | source /dev/stdin

#export LD_LIBRARY_PATH=/shared/apps/cuda7.0/lib64:$LD_LIBRARY_PATH

# OpenMP environment variables
export OMP_NUM_THREADS=$((%ncores%*2))
export OMP_SCHEDULE="dynamic"
export OMP_NESTED=FALSE
export OMP_STACKSIZE="20M"

# Set stack size unlimited
ulimit -s unlimited

# Run binaries
if [ "$usegraphlist" -eq 1 ] ; then # Use a list of graph IDs
  graphfile=graphlist
  readarray -t graphs < ${graphfile}
  for (( i=1; i<=${#graphs[@]}; i++ )) ; do
    echo "Reading graph ${graphs[$i-1]}."
    if [ %nnodes% -eq 1 ] ; then # Regular run
      ./CausalSet_%partition% %flags% --graph ${graphs[$i-1]} 
    else # Use MPI
      mpirun -prot -srun ./CausalSet_%partition% %flags% --graph ${graphs[$i-1]} 
    fi
    sleep 1
  done
else # Use a single graph id or create new graphs
  for (( i=1; i<=%nsamples%; i++ )) ; do
    echo "Starting Trial $i of %nsamples%..."
    if [ %nnodes% -eq 1 ] ; then # Regular run
      ./CausalSet_%partition% %flags% --graph %readgraphid% 
    else # Use MPI
      mpirun -prot -srun ./CausalSet_%partition% %flags% --graph %readgraphid% 
    fi
    sleep 1
  done
fi

wait

# Clean up
if [ ${usegraphlist} -eq 1 ] ; then
  rm $graphfile
fi
